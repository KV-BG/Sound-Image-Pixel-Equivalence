{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e78727a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "\n",
    "def generate_amplitude_image(file_path, grid_size=64):\n",
    "    \"\"\"\n",
    "    Generates a grayscale image from an audio file based on the\n",
    "    average amplitude of its segments.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The full path to the audio file.\n",
    "        grid_size (int): The dimension of the grid (e.g., 16x16).\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: A 3-channel (RGB) grayscale image, or None if error.\n",
    "    \"\"\"\n",
    "    \n",
    "    num_segments = grid_size * grid_size # 16*16 = 256\n",
    "    \n",
    "    try:\n",
    "        # 1. Get the audio file\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "    except Exception as e:\n",
    "        print(f\"  [Error] Skipping {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "    \n",
    "    # Check for silent/empty files\n",
    "    if len(y) == 0:\n",
    "        print(f\"  [Warning] Skipping empty file: {os.path.basename(file_path)}\")\n",
    "        return None\n",
    "        \n",
    "    # We use the absolute value for amplitude\n",
    "    y_abs = np.abs(y)\n",
    "    \n",
    "    # 2. Divide it into 256 equal segments\n",
    "    segments = np.array_split(y_abs, num_segments)\n",
    "    \n",
    "    # 3. Calculate the average amplitude for each segment\n",
    "    avg_amplitudes = [np.mean(segment) for segment in segments]\n",
    "    amplitudes_array = np.array(avg_amplitudes)\n",
    "    \n",
    "    # 4. Normalize the amplitudes to a 0-255 scale\n",
    "    min_amp = np.min(amplitudes_array)\n",
    "    max_amp = np.max(amplitudes_array)\n",
    "    range_amp = max_amp - min_amp\n",
    "    \n",
    "    if range_amp == 0:\n",
    "        # Avoid division by zero if the audio is silent or constant\n",
    "        normalized_amplitudes = np.zeros(num_segments, dtype=np.uint8)\n",
    "    else:\n",
    "        # Min-max normalization\n",
    "        normalized_amplitudes = ((amplitudes_array - min_amp) / range_amp) * 255.0\n",
    "        normalized_amplitudes = normalized_amplitudes.astype(np.uint8)\n",
    "    \n",
    "    # 5. & 6. Arrange pixels in a row-wise grid\n",
    "    \n",
    "    # Reshape the 1D array of 256 values into a 16x16 grid\n",
    "    pixel_grid_1channel = normalized_amplitudes.reshape(grid_size, grid_size)\n",
    "    \n",
    "    # Create a 3-channel RGB image where R=G=B=y\n",
    "    pixel_grid_3channel = np.stack([pixel_grid_1channel] * 3, axis=-1)\n",
    "    \n",
    "    # Create the image from the 3-channel NumPy array\n",
    "    img = Image.fromarray(pixel_grid_3channel, 'RGB')\n",
    "    \n",
    "    return img\n",
    "\n",
    "def process_all_genres(base_audio_dir, base_output_dir, grid_size=64):\n",
    "    \"\"\"\n",
    "    Finds all genre sub-folders, loops through all .wav files,\n",
    "    generates images, and saves them in a mirrored directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting batch processing...\")\n",
    "    print(f\"Input directory:  {base_audio_dir}\")\n",
    "    print(f\"Output directory: {base_output_dir}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_files_processed = 0\n",
    "    \n",
    "    # Find all subdirectories (genres) in the base audio directory\n",
    "    try:\n",
    "        genre_dirs = [d.name for d in os.scandir(base_audio_dir) if d.is_dir()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {base_audio_dir}\")\n",
    "        print(\"Please check your 'base_audio_dir' variable.\")\n",
    "        return\n",
    "\n",
    "    if not genre_dirs:\n",
    "        print(f\"ERROR: No genre sub-folders found in {base_audio_dir}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(genre_dirs)} genres: {genre_dirs}\")\n",
    "\n",
    "    # Loop over each genre folder\n",
    "    for genre in genre_dirs:\n",
    "        input_genre_path = os.path.join(base_audio_dir, genre)\n",
    "        output_genre_path = os.path.join(base_output_dir, genre)\n",
    "        \n",
    "        # 1. Create the output directory (e.g., \"amplitude_images/classical\")\n",
    "        os.makedirs(output_genre_path, exist_ok=True)\n",
    "        \n",
    "        # 2. Find all .wav files in the input genre directory\n",
    "        audio_files = glob.glob(os.path.join(input_genre_path, \"*.wav\"))\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"\\nNo .wav files found for genre: {genre}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {len(audio_files)} files for genre: {genre}...\")\n",
    "        files_in_genre = 0\n",
    "        \n",
    "        # 3. Loop over each file, generate image, and save\n",
    "        for file_path in audio_files:\n",
    "            # Generate the image\n",
    "            amplitude_image = generate_amplitude_image(file_path, grid_size)\n",
    "            \n",
    "            if amplitude_image:\n",
    "                # Create a new filename\n",
    "                # e.g., \"classical.00001.wav\" -> \"classical.00001.png\"\n",
    "                base_filename = os.path.basename(file_path)\n",
    "                filename_no_ext = os.path.splitext(base_filename)[0]\n",
    "                output_filename = f\"{filename_no_ext}.png\"\n",
    "                \n",
    "                # Create the full output path\n",
    "                output_path = os.path.join(output_genre_path, output_filename)\n",
    "                \n",
    "                # Save the image\n",
    "                amplitude_image.save(output_path)\n",
    "                files_in_genre += 1\n",
    "\n",
    "        print(f\"Finished {genre}: {files_in_genre} images saved.\")\n",
    "        total_files_processed += files_in_genre\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"---\")\n",
    "    print(f\"âœ… All processing complete.\")\n",
    "    print(f\"Total files processed: {total_files_processed}\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Your images are saved in the '{base_output_dir}' directory.\")\n",
    "\n",
    "\n",
    "# --- ğŸš€ HOW TO RUN ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Define your main audio directory\n",
    "    # This is the folder that CONTAINS \"classical\", \"blues\", \"rock\", etc.\n",
    "    # ****** PLEASE UPDATE THIS PATH ******\n",
    "    base_audio_dir = \"Data/genres_original/\" \n",
    "    \n",
    "    # 2. Define where you want to save the new images\n",
    "    # A new folder \"amplitude_images\" will be created.\n",
    "    base_output_dir = \"amplitude_images_64/\"\n",
    "    \n",
    "    # 3. Run the processing\n",
    "    process_all_genres(base_audio_dir, base_output_dir, grid_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "318208e5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 64x64 images from 'amplitude_images_64/'...\n",
      "\n",
      "Total samples loaded: 999\n",
      "Training data shape: (799, 64, 64, 3)\n",
      "Testing data shape: (200, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)    â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                   â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d (\u001b[38;5;33mConv2D\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d (\u001b[38;5;33mMaxPooling2D\u001b[0m)    â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_1 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_1 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten (\u001b[38;5;33mFlatten\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense (\u001b[38;5;33mDense\u001b[0m)                   â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚     \u001b[38;5;34m1,605,760\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_1 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚         \u001b[38;5;34m1,290\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "Epoch 1/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 77ms/step - accuracy: 0.0876 - loss: 2.3160 - val_accuracy: 0.1550 - val_loss: 2.2720\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.1577 - loss: 2.2404 - val_accuracy: 0.1600 - val_loss: 2.2349\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 77ms/step - accuracy: 0.2641 - loss: 2.0821 - val_accuracy: 0.2250 - val_loss: 2.1076\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.3655 - loss: 1.8262 - val_accuracy: 0.2100 - val_loss: 2.1837\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.4756 - loss: 1.5453 - val_accuracy: 0.2300 - val_loss: 2.1319\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.6483 - loss: 1.1992 - val_accuracy: 0.2550 - val_loss: 2.2376\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.7559 - loss: 0.8286 - val_accuracy: 0.2950 - val_loss: 2.4370\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.8598 - loss: 0.5701 - val_accuracy: 0.2850 - val_loss: 2.4757\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9024 - loss: 0.4230 - val_accuracy: 0.2600 - val_loss: 2.7341\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.9524 - loss: 0.2460 - val_accuracy: 0.2750 - val_loss: 2.9887\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9725 - loss: 0.1674 - val_accuracy: 0.2950 - val_loss: 2.8820\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9862 - loss: 0.0989 - val_accuracy: 0.2900 - val_loss: 3.0970\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9962 - loss: 0.0459 - val_accuracy: 0.2700 - val_loss: 3.4431\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9975 - loss: 0.0308 - val_accuracy: 0.2850 - val_loss: 3.4680\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9987 - loss: 0.0224 - val_accuracy: 0.2650 - val_loss: 3.7223\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9987 - loss: 0.0249 - val_accuracy: 0.2750 - val_loss: 3.7112\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9987 - loss: 0.0197 - val_accuracy: 0.2850 - val_loss: 3.6819\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 76ms/step - accuracy: 0.9975 - loss: 0.0220 - val_accuracy: 0.2700 - val_loss: 3.7531\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 72ms/step - accuracy: 0.9987 - loss: 0.0175 - val_accuracy: 0.3100 - val_loss: 3.7946\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9987 - loss: 0.0164 - val_accuracy: 0.2650 - val_loss: 4.0338\n",
      "Epoch 21/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 79ms/step - accuracy: 0.9975 - loss: 0.0147 - val_accuracy: 0.2700 - val_loss: 3.9727\n",
      "Epoch 22/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 73ms/step - accuracy: 0.9975 - loss: 0.0157 - val_accuracy: 0.2950 - val_loss: 3.9369\n",
      "Epoch 23/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 69ms/step - accuracy: 0.9975 - loss: 0.0188 - val_accuracy: 0.2850 - val_loss: 4.1192\n",
      "Epoch 24/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9975 - loss: 0.0227 - val_accuracy: 0.2750 - val_loss: 4.0557\n",
      "Epoch 25/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 66ms/step - accuracy: 0.9950 - loss: 0.0344 - val_accuracy: 0.2750 - val_loss: 4.0017\n",
      "Epoch 26/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9950 - loss: 0.0254 - val_accuracy: 0.2450 - val_loss: 3.9367\n",
      "Epoch 27/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 67ms/step - accuracy: 0.9912 - loss: 0.0353 - val_accuracy: 0.2700 - val_loss: 3.9614\n",
      "Epoch 28/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9975 - loss: 0.0226 - val_accuracy: 0.2650 - val_loss: 4.0838\n",
      "Epoch 29/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 65ms/step - accuracy: 0.9987 - loss: 0.0102 - val_accuracy: 0.2500 - val_loss: 4.3183\n",
      "Epoch 30/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m2s\u001b[0m 68ms/step - accuracy: 0.9975 - loss: 0.0165 - val_accuracy: 0.2500 - val_loss: 4.1472\n",
      "\n",
      "Evaluating on test data...\n",
      "7/7 - 0s - 19ms/step - accuracy: 0.2500 - loss: 4.1472\n",
      "------------------------------\n",
      "âœ… Final Test Accuracy: 25.00%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# This MUST match the 'base_output_dir' from your generation script\n",
    "IMAGE_DIR = \"amplitude_images_64/\" \n",
    "CATEGORIES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] # Make sure this matches your folder names\n",
    "IMG_SIZE = 64 # This MUST match the 'grid_size' from your script\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "print(f\"Loading {IMG_SIZE}x{IMG_SIZE} images from '{IMAGE_DIR}'...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i, category in enumerate(CATEGORIES):\n",
    "    path = os.path.join(IMAGE_DIR, category)\n",
    "    try:\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            \n",
    "            # Load the image in COLOR (since your script saved as RGB)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR) \n",
    "            \n",
    "            # Resize just in case, though your script should handle this\n",
    "            img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            data.append(img_resized)\n",
    "            labels.append(i) # 0 for blues, 1 for rock, 2 for nature\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading images from {path}: {e}\")\n",
    "        print(\"Please double-check your 'IMAGE_DIR' and 'CATEGORIES'\")\n",
    "\n",
    "if not data:\n",
    "    print(\"\\nError: No data loaded. Stopping.\")\n",
    "else:\n",
    "    # --- 3. Preprocessing ---\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Normalize pixel values from 0-255 to 0.0-1.0\n",
    "    # This is critical for neural network performance\n",
    "    X = X / 255.0\n",
    "\n",
    "    # --- 4. Split Data (80% train, 20% test) ---\n",
    "    # stratify=y ensures the 80/20 split has a fair mix of all 3 classes\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Training data shape: {X_train.shape}\") # Should be (240, 64, 64, 3)\n",
    "    print(f\"Testing data shape: {X_test.shape}\")   # Should be (60, 64, 64, 3)\n",
    "\n",
    "    # --- 5. Build the CNN Model ---\n",
    "    # For 64x64 images, we'll use two Conv/Pool blocks\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))) \n",
    "    \n",
    "    # Block 1:\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Block 2:\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten: Squashes 2D data to 1D\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # Hidden layer:\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu')) # Increased size for 64x64\n",
    "    \n",
    "    # Output layer: 3 neurons (1 per class), softmax for probabilities\n",
    "    model.add(tf.keras.layers.Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # --- 6. Compile the Model ---\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy', # Use this for integer labels (0, 1, 2)\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # --- 7. Train the Model ---\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30, # 30-50 epochs is a good starting point\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- 8. Evaluate ---\n",
    "    print(\"\\nEvaluating on test data...\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(\"---\" * 10)\n",
    "    print(f\"âœ… Final Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    print(\"---\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c288330a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting batch processing...\n",
      "Input directory:  Data/genres_original/\n",
      "Output directory: hsv_images_64/\n",
      "---\n",
      "Found 10 genres: ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock']\n",
      "\n",
      "Processing 100 files for genre: blues...\n",
      "Finished blues: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: classical...\n",
      "Finished classical: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: country...\n",
      "Finished country: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: disco...\n",
      "Finished disco: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: hiphop...\n",
      "Finished hiphop: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: jazz...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\geeta\\AppData\\Local\\Temp\\ipykernel_21020\\82968464.py:36: UserWarning: PySoundFile failed. Trying audioread instead.\n",
      "  y, sr = librosa.load(file_path, sr=None)\n",
      "c:\\Users\\geeta\\AppData\\Local\\Programs\\Python\\Python313\\Lib\\site-packages\\librosa\\core\\audio.py:184: FutureWarning: librosa.core.audio.__audioread_load\n",
      "\tDeprecated as of librosa version 0.10.0.\n",
      "\tIt will be removed in librosa version 1.0.\n",
      "  y, sr_native = __audioread_load(path, offset, duration, dtype)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   [Error] Skipping jazz.00054.wav: \n",
      "Finished jazz: 99 images saved.\n",
      "\n",
      "Processing 100 files for genre: metal...\n",
      "Finished metal: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: pop...\n",
      "Finished pop: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: reggae...\n",
      "Finished reggae: 100 images saved.\n",
      "\n",
      "Processing 100 files for genre: rock...\n",
      "Finished rock: 100 images saved.\n",
      "---\n",
      "âœ… All processing complete.\n",
      "Total files processed: 999\n",
      "Total time taken: 298.54 seconds\n",
      "Your images are saved in the 'hsv_images_64/' directory.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "from PIL import Image\n",
    "import os\n",
    "import glob\n",
    "import time\n",
    "import cv2 # We need OpenCV for image resizing\n",
    "\n",
    "def normalize_map(data):\n",
    "    \"\"\"Normalizes a 2D numpy array to the 0.0 - 1.0 range.\"\"\"\n",
    "    min_val = np.min(data)\n",
    "    max_val = np.max(data)\n",
    "    range_val = max_val - min_val\n",
    "    \n",
    "    if range_val == 0:\n",
    "        return np.zeros(data.shape, dtype=np.float32)\n",
    "    \n",
    "    return (data - min_val) / range_val\n",
    "\n",
    "def generate_hsv_image(file_path, grid_size=64):\n",
    "    \"\"\"\n",
    "    Generates a 64x64 HSV image from an audio file by mapping\n",
    "    Loudness -> Value\n",
    "    Pitch     -> Hue\n",
    "    Timbre    -> Saturation\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): The full path to the audio file.\n",
    "        grid_size (int): The dimension of the grid (e.g., 64).\n",
    "        \n",
    "    Returns:\n",
    "        PIL.Image: A 3-channel RGB image, or None if error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Load Audio\n",
    "        y, sr = librosa.load(file_path, sr=None)\n",
    "        \n",
    "        if len(y) == 0:\n",
    "            print(f\"   [Warning] Skipping empty file: {os.path.basename(file_path)}\")\n",
    "            return None\n",
    "            \n",
    "        # 2. Get the full-resolution STFT\n",
    "        \n",
    "        # Calculate the complex STFT\n",
    "        D_complex = librosa.stft(y, n_fft=2048) # <-- CHANGED\n",
    "        \n",
    "        # Get the magnitude spectrogram (what we need for features)\n",
    "        S_full = np.abs(D_complex) # <-- CHANGED\n",
    "\n",
    "        # --- 3. CALCULATE THE 3 FEATURE MAPS ---\n",
    "\n",
    "        # VALUE (Loudness)\n",
    "        # We use a Mel Spectrogram with 'grid_size' (64) bins\n",
    "        S_mel = librosa.feature.melspectrogram(y=y, sr=sr, n_mels=grid_size, n_fft=2048)\n",
    "        # Convert to log scale (dB) for perceptual loudness\n",
    "        V_map = librosa.amplitude_to_db(S_mel, ref=np.max)\n",
    "        \n",
    "        # HUE (Pitch / Spectral \"Brightness\")\n",
    "        # We use Spectral Centroid, which finds the \"center of mass\" of the spectrum\n",
    "        H_map_raw = librosa.feature.spectral_centroid(S=S_full, sr=sr)\n",
    "        \n",
    "        # SATURATION (Timbre / Tonal Purity)\n",
    "        # We use Spectral Flatness. \n",
    "        # High flatness (near 1.0) = Noisy (e.g., white noise)\n",
    "        # Low flatness (near 0.0) = Tonal (e.g., a sine wave)\n",
    "        S_map_raw = librosa.feature.spectral_flatness(S=S_full)\n",
    "\n",
    "        # --- 4. NORMALIZE & ALIGN MAPS ---\n",
    "\n",
    "        # We need all maps to be (64, N_frames)\n",
    "        # V_map is already (64, N_frames)\n",
    "        V_map_norm = normalize_map(V_map) # Normalize 0.0-1.0\n",
    "\n",
    "        # H_map and S_map are (1, N_frames). We repeat them 64 times\n",
    "        # to match the V_map's height.\n",
    "        \n",
    "        # Normalize H_map (0-1)\n",
    "        H_map_norm = normalize_map(H_map_raw)\n",
    "        H_map_full = np.tile(H_map_norm, (grid_size, 1))\n",
    "        \n",
    "        # Normalize S_map (0-1) and INVERT it\n",
    "        # We want tonal (low flatness) to be HIGH saturation\n",
    "        S_map_norm_inv = 1.0 - normalize_map(S_map_raw)\n",
    "        S_map_full = np.tile(S_map_norm_inv, (grid_size, 1))\n",
    "        \n",
    "        # --- 5. CREATE HSV IMAGE & RESIZE ---\n",
    "        \n",
    "        # Stack the 3 maps to create an HSV image\n",
    "        # Shape is (64, N_frames, 3)\n",
    "        hsv_image_long = np.stack([H_map_full, S_map_full, V_map_norm], axis=-1)\n",
    "\n",
    "        # Resize the \"time\" axis (N_frames) to be 'grid_size' (64)\n",
    "        # This squashes or stretches the audio to fit the 64-pixel width\n",
    "        hsv_image_64x64 = cv2.resize(\n",
    "            hsv_image_long, \n",
    "            (grid_size, grid_size), \n",
    "            interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        \n",
    "        # --- 6. CONVERT TO RGB & SAVE ---\n",
    "        \n",
    "        # Convert from 0.0-1.0 (float) to 0-255 (uint8)\n",
    "        hsv_image_uint8 = (hsv_image_64x64 * 255).astype(np.uint8)\n",
    "\n",
    "        # Convert from HSV color space to RGB\n",
    "        rgb_image_array = cv2.cvtColor(hsv_image_uint8, cv2.COLOR_HSV2RGB)\n",
    "        \n",
    "        # Create a PIL Image from the numpy array\n",
    "        img = Image.fromarray(rgb_image_array, 'RGB')\n",
    "        \n",
    "        return img\n",
    "\n",
    "    except Exception as e:\n",
    "        # Catch other potential errors\n",
    "        print(f\"   [Error] Skipping {os.path.basename(file_path)}: {e}\")\n",
    "        return None\n",
    "\n",
    "\n",
    "def process_all_genres(base_audio_dir, base_output_dir, grid_size=64):\n",
    "    \"\"\"\n",
    "    (This is the same helper function from your script)\n",
    "    Finds all genre sub-folders, loops through all .wav files,\n",
    "    generates images, and saves them in a mirrored directory structure.\n",
    "    \"\"\"\n",
    "    \n",
    "    print(f\"Starting batch processing...\")\n",
    "    print(f\"Input directory:  {base_audio_dir}\")\n",
    "    print(f\"Output directory: {base_output_dir}\")\n",
    "    print(\"---\")\n",
    "    \n",
    "    start_time = time.time()\n",
    "    total_files_processed = 0\n",
    "    \n",
    "    try:\n",
    "        genre_dirs = [d.name for d in os.scandir(base_audio_dir) if d.is_dir()]\n",
    "    except FileNotFoundError:\n",
    "        print(f\"ERROR: Input directory not found: {base_audio_dir}\")\n",
    "        print(\"Please check your 'base_audio_dir' variable.\")\n",
    "        return\n",
    "\n",
    "    if not genre_dirs:\n",
    "        print(f\"ERROR: No genre sub-folders found in {base_audio_dir}.\")\n",
    "        return\n",
    "\n",
    "    print(f\"Found {len(genre_dirs)} genres: {genre_dirs}\")\n",
    "\n",
    "    # Loop over each genre folder\n",
    "    for genre in genre_dirs:\n",
    "        input_genre_path = os.path.join(base_audio_dir, genre)\n",
    "        output_genre_path = os.path.join(base_output_dir, genre)\n",
    "        \n",
    "        os.makedirs(output_genre_path, exist_ok=True)\n",
    "        \n",
    "        audio_files = glob.glob(os.path.join(input_genre_path, \"*.wav\"))\n",
    "        \n",
    "        if not audio_files:\n",
    "            print(f\"\\nNo .wav files found for genre: {genre}. Skipping.\")\n",
    "            continue\n",
    "\n",
    "        print(f\"\\nProcessing {len(audio_files)} files for genre: {genre}...\")\n",
    "        files_in_genre = 0\n",
    "        \n",
    "        # Loop over each file, generate image, and save\n",
    "        for file_path in audio_files:\n",
    "            # Generate the new HSV image\n",
    "            hsv_image = generate_hsv_image(file_path, grid_size)\n",
    "            \n",
    "            if hsv_image:\n",
    "                base_filename = os.path.basename(file_path)\n",
    "                filename_no_ext = os.path.splitext(base_filename)[0]\n",
    "                output_filename = f\"{filename_no_ext}.png\"\n",
    "                output_path = os.path.join(output_genre_path, output_filename)\n",
    "                \n",
    "                # Save the image\n",
    "                hsv_image.save(output_path)\n",
    "                files_in_genre += 1\n",
    "\n",
    "        print(f\"Finished {genre}: {files_in_genre} images saved.\")\n",
    "        total_files_processed += files_in_genre\n",
    "\n",
    "    end_time = time.time()\n",
    "    print(\"---\")\n",
    "    print(f\"âœ… All processing complete.\")\n",
    "    print(f\"Total files processed: {total_files_processed}\")\n",
    "    print(f\"Total time taken: {end_time - start_time:.2f} seconds\")\n",
    "    print(f\"Your images are saved in the '{base_output_dir}' directory.\")\n",
    "\n",
    "\n",
    "# --- ğŸš€ HOW TO RUN ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Define your main audio directory\n",
    "    # This is the folder that CONTAINS \"classical\", \"blues\", \"rock\", etc.\n",
    "    # ****** PLEASE UPDATE THIS PATH ******\n",
    "    base_audio_dir = \"Data/genres_original/\" \n",
    "    \n",
    "    # 2. Define where you want to save the new images\n",
    "    # A new folder \"hsv_images_64\" will be created.\n",
    "    base_output_dir = \"hsv_images_64/\"\n",
    "    \n",
    "    # 3. Run the processing\n",
    "    process_all_genres(base_audio_dir, base_output_dir, grid_size=64)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "055cc268",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading 64x64 images from 'hsv_images_64/'...\n",
      "\n",
      "Total samples loaded: 999\n",
      "Training data shape: (799, 64, 64, 3)\n",
      "Testing data shape: (200, 64, 64, 3)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential_1\"</span>\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1mModel: \"sequential_1\"\u001b[0m\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ<span style=\"font-weight: bold\"> Layer (type)                    </span>â”ƒ<span style=\"font-weight: bold\"> Output Shape           </span>â”ƒ<span style=\"font-weight: bold\">       Param # </span>â”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">62</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚           <span style=\"color: #00af00; text-decoration-color: #00af00\">896</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">31</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">32</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Conv2D</span>)               â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">29</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚        <span style=\"color: #00af00; text-decoration-color: #00af00\">18,496</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">MaxPooling2D</span>)  â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">14</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">64</span>)     â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Flatten</span>)             â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">12544</span>)          â”‚             <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">128</span>)            â”‚     <span style=\"color: #00af00; text-decoration-color: #00af00\">1,605,760</span> â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                 â”‚ (<span style=\"color: #00d7ff; text-decoration-color: #00d7ff\">None</span>, <span style=\"color: #00af00; text-decoration-color: #00af00\">10</span>)             â”‚         <span style=\"color: #00af00; text-decoration-color: #00af00\">1,290</span> â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n",
       "</pre>\n"
      ],
      "text/plain": [
       "â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”³â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”“\n",
       "â”ƒ\u001b[1m \u001b[0m\u001b[1mLayer (type)                   \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1mOutput Shape          \u001b[0m\u001b[1m \u001b[0mâ”ƒ\u001b[1m \u001b[0m\u001b[1m      Param #\u001b[0m\u001b[1m \u001b[0mâ”ƒ\n",
       "â”¡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â•‡â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”©\n",
       "â”‚ conv2d_2 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m62\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚           \u001b[38;5;34m896\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_2 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m31\u001b[0m, \u001b[38;5;34m32\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ conv2d_3 (\u001b[38;5;33mConv2D\u001b[0m)               â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m29\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚        \u001b[38;5;34m18,496\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ max_pooling2d_3 (\u001b[38;5;33mMaxPooling2D\u001b[0m)  â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m14\u001b[0m, \u001b[38;5;34m64\u001b[0m)     â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ flatten_1 (\u001b[38;5;33mFlatten\u001b[0m)             â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m12544\u001b[0m)          â”‚             \u001b[38;5;34m0\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_2 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m128\u001b[0m)            â”‚     \u001b[38;5;34m1,605,760\u001b[0m â”‚\n",
       "â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤\n",
       "â”‚ dense_3 (\u001b[38;5;33mDense\u001b[0m)                 â”‚ (\u001b[38;5;45mNone\u001b[0m, \u001b[38;5;34m10\u001b[0m)             â”‚         \u001b[38;5;34m1,290\u001b[0m â”‚\n",
       "â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”´â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Total params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">1,626,442</span> (6.20 MB)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m1,626,442\u001b[0m (6.20 MB)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n",
       "</pre>\n"
      ],
      "text/plain": [
       "\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Training model...\n",
      "Epoch 1/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m11s\u001b[0m 184ms/step - accuracy: 0.1727 - loss: 2.1796 - val_accuracy: 0.2750 - val_loss: 2.0587\n",
      "Epoch 2/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - accuracy: 0.3091 - loss: 1.8501 - val_accuracy: 0.3750 - val_loss: 1.8510\n",
      "Epoch 3/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 155ms/step - accuracy: 0.5144 - loss: 1.4897 - val_accuracy: 0.3750 - val_loss: 1.8582\n",
      "Epoch 4/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 156ms/step - accuracy: 0.6095 - loss: 1.1898 - val_accuracy: 0.4200 - val_loss: 1.7571\n",
      "Epoch 5/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.6496 - loss: 1.0285 - val_accuracy: 0.4650 - val_loss: 1.5250\n",
      "Epoch 6/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - accuracy: 0.7835 - loss: 0.7361 - val_accuracy: 0.4800 - val_loss: 1.6634\n",
      "Epoch 7/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - accuracy: 0.8373 - loss: 0.5579 - val_accuracy: 0.4950 - val_loss: 1.6301\n",
      "Epoch 8/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 151ms/step - accuracy: 0.9049 - loss: 0.4014 - val_accuracy: 0.4700 - val_loss: 1.6669\n",
      "Epoch 9/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.9474 - loss: 0.2623 - val_accuracy: 0.5100 - val_loss: 1.6404\n",
      "Epoch 10/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 144ms/step - accuracy: 0.9800 - loss: 0.1626 - val_accuracy: 0.4900 - val_loss: 1.7545\n",
      "Epoch 11/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - accuracy: 0.9937 - loss: 0.0870 - val_accuracy: 0.5050 - val_loss: 1.8909\n",
      "Epoch 12/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 135ms/step - accuracy: 1.0000 - loss: 0.0524 - val_accuracy: 0.5300 - val_loss: 1.9540\n",
      "Epoch 13/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 134ms/step - accuracy: 0.9987 - loss: 0.0330 - val_accuracy: 0.5000 - val_loss: 2.0324\n",
      "Epoch 14/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 142ms/step - accuracy: 0.9950 - loss: 0.0370 - val_accuracy: 0.4800 - val_loss: 2.2471\n",
      "Epoch 15/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.9962 - loss: 0.0345 - val_accuracy: 0.5300 - val_loss: 2.0804\n",
      "Epoch 16/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 146ms/step - accuracy: 0.9975 - loss: 0.0235 - val_accuracy: 0.5050 - val_loss: 2.0887\n",
      "Epoch 17/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 136ms/step - accuracy: 0.9962 - loss: 0.0226 - val_accuracy: 0.5150 - val_loss: 2.1619\n",
      "Epoch 18/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 149ms/step - accuracy: 0.9987 - loss: 0.0212 - val_accuracy: 0.5200 - val_loss: 2.1483\n",
      "Epoch 19/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 147ms/step - accuracy: 0.9987 - loss: 0.0193 - val_accuracy: 0.5300 - val_loss: 2.1206\n",
      "Epoch 20/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 138ms/step - accuracy: 0.9962 - loss: 0.0163 - val_accuracy: 0.5100 - val_loss: 2.1888\n",
      "Epoch 21/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 139ms/step - accuracy: 0.9987 - loss: 0.0139 - val_accuracy: 0.5250 - val_loss: 2.2571\n",
      "Epoch 22/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m3s\u001b[0m 136ms/step - accuracy: 0.9975 - loss: 0.0180 - val_accuracy: 0.5400 - val_loss: 2.2659\n",
      "Epoch 23/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 141ms/step - accuracy: 0.9975 - loss: 0.0208 - val_accuracy: 0.5200 - val_loss: 2.3496\n",
      "Epoch 24/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 143ms/step - accuracy: 0.9975 - loss: 0.0268 - val_accuracy: 0.5250 - val_loss: 2.3692\n",
      "Epoch 25/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 138ms/step - accuracy: 0.9987 - loss: 0.0148 - val_accuracy: 0.5100 - val_loss: 2.2061\n",
      "Epoch 26/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 153ms/step - accuracy: 0.9987 - loss: 0.0126 - val_accuracy: 0.5200 - val_loss: 2.2970\n",
      "Epoch 27/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 217ms/step - accuracy: 0.9987 - loss: 0.0151 - val_accuracy: 0.4950 - val_loss: 2.3263\n",
      "Epoch 28/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 150ms/step - accuracy: 0.9987 - loss: 0.0117 - val_accuracy: 0.5400 - val_loss: 2.3301\n",
      "Epoch 29/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 137ms/step - accuracy: 0.9987 - loss: 0.0094 - val_accuracy: 0.5600 - val_loss: 2.2940\n",
      "Epoch 30/30\n",
      "\u001b[1m25/25\u001b[0m \u001b[32mâ”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”â”\u001b[0m\u001b[37m\u001b[0m \u001b[1m4s\u001b[0m 140ms/step - accuracy: 0.9987 - loss: 0.0117 - val_accuracy: 0.5200 - val_loss: 2.2547\n",
      "\n",
      "Evaluating on test data...\n",
      "7/7 - 0s - 55ms/step - accuracy: 0.5200 - loss: 2.2547\n",
      "------------------------------\n",
      "âœ… Final Test Accuracy: 52.00%\n",
      "------------------------------\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import cv2\n",
    "import os\n",
    "import tensorflow as tf\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# --- 1. Configuration ---\n",
    "# This MUST match the 'base_output_dir' from your generation script\n",
    "IMAGE_DIR = \"hsv_images_64/\" \n",
    "CATEGORIES = ['blues', 'classical', 'country', 'disco', 'hiphop', 'jazz', 'metal', 'pop', 'reggae', 'rock'] # Make sure this matches your folder names\n",
    "IMG_SIZE = 64 # This MUST match the 'grid_size' from your script\n",
    "\n",
    "# --- 2. Load Data ---\n",
    "print(f\"Loading {IMG_SIZE}x{IMG_SIZE} images from '{IMAGE_DIR}'...\")\n",
    "data = []\n",
    "labels = []\n",
    "\n",
    "for i, category in enumerate(CATEGORIES):\n",
    "    path = os.path.join(IMAGE_DIR, category)\n",
    "    try:\n",
    "        for img_name in os.listdir(path):\n",
    "            img_path = os.path.join(path, img_name)\n",
    "            \n",
    "            # Load the image in COLOR (since your script saved as RGB)\n",
    "            img = cv2.imread(img_path, cv2.IMREAD_COLOR) \n",
    "            \n",
    "            # Resize just in case, though your script should handle this\n",
    "            img_resized = cv2.resize(img, (IMG_SIZE, IMG_SIZE))\n",
    "            data.append(img_resized)\n",
    "            labels.append(i) # 0 for blues, 1 for rock, 2 for nature\n",
    "    except Exception as e:\n",
    "        print(f\"Error loading images from {path}: {e}\")\n",
    "        print(\"Please double-check your 'IMAGE_DIR' and 'CATEGORIES'\")\n",
    "\n",
    "if not data:\n",
    "    print(\"\\nError: No data loaded. Stopping.\")\n",
    "else:\n",
    "    # --- 3. Preprocessing ---\n",
    "    # Convert to numpy arrays\n",
    "    X = np.array(data)\n",
    "    y = np.array(labels)\n",
    "\n",
    "    # Normalize pixel values from 0-255 to 0.0-1.0\n",
    "    # This is critical for neural network performance\n",
    "    X = X / 255.0\n",
    "\n",
    "    # --- 4. Split Data (80% train, 20% test) ---\n",
    "    # stratify=y ensures the 80/20 split has a fair mix of all 3 classes\n",
    "    X_train, X_test, y_train, y_test = train_test_split(\n",
    "        X, y, test_size=0.2, random_state=42, stratify=y\n",
    "    )\n",
    "\n",
    "    print(f\"\\nTotal samples loaded: {len(X)}\")\n",
    "    print(f\"Training data shape: {X_train.shape}\") # Should be (240, 64, 64, 3)\n",
    "    print(f\"Testing data shape: {X_test.shape}\")   # Should be (60, 64, 64, 3)\n",
    "\n",
    "    # --- 5. Build the CNN Model ---\n",
    "    # For 64x64 images, we'll use two Conv/Pool blocks\n",
    "    \n",
    "    model = tf.keras.models.Sequential()\n",
    "    \n",
    "    # Input layer\n",
    "    model.add(tf.keras.layers.Input(shape=(IMG_SIZE, IMG_SIZE, 3))) \n",
    "    \n",
    "    # Block 1:\n",
    "    model.add(tf.keras.layers.Conv2D(32, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Block 2:\n",
    "    model.add(tf.keras.layers.Conv2D(64, (3, 3), activation='relu'))\n",
    "    model.add(tf.keras.layers.MaxPooling2D((2, 2)))\n",
    "    \n",
    "    # Flatten: Squashes 2D data to 1D\n",
    "    model.add(tf.keras.layers.Flatten())\n",
    "    \n",
    "    # Hidden layer:\n",
    "    model.add(tf.keras.layers.Dense(128, activation='relu')) # Increased size for 64x64\n",
    "    \n",
    "    # Output layer: 3 neurons (1 per class), softmax for probabilities\n",
    "    model.add(tf.keras.layers.Dense(len(CATEGORIES), activation='softmax'))\n",
    "\n",
    "    model.summary()\n",
    "\n",
    "    # --- 6. Compile the Model ---\n",
    "    model.compile(\n",
    "        optimizer='adam',\n",
    "        loss='sparse_categorical_crossentropy', # Use this for integer labels (0, 1, 2)\n",
    "        metrics=['accuracy']\n",
    "    )\n",
    "\n",
    "    # --- 7. Train the Model ---\n",
    "    print(\"\\nTraining model...\")\n",
    "    history = model.fit(\n",
    "        X_train, y_train,\n",
    "        epochs=30, # 30-50 epochs is a good starting point\n",
    "        validation_data=(X_test, y_test),\n",
    "        verbose=1\n",
    "    )\n",
    "\n",
    "    # --- 8. Evaluate ---\n",
    "    print(\"\\nEvaluating on test data...\")\n",
    "    test_loss, test_acc = model.evaluate(X_test, y_test, verbose=2)\n",
    "    print(\"---\" * 10)\n",
    "    print(f\"âœ… Final Test Accuracy: {test_acc * 100:.2f}%\")\n",
    "    print(\"---\" * 10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "af2a0d9b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reconstruct audio from: hsv_images_64/rock/rock.00000.png\n",
      "   Running Griffin-Lim... (this can take a moment)\n",
      "\n",
      "âœ… Success! Reconstructed audio saved to:\n",
      "   reconstructed_audio_test.wav\n",
      "\n",
      "Listen to it! Compare it to the original 'rock.00000.wav'.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import cv2\n",
    "import soundfile as sf  # We use soundfile to save the audio\n",
    "from PIL import Image\n",
    "\n",
    "def generate_audio_from_image(image_path, n_fft=2048, grid_size=64, sr=22050):\n",
    "    \"\"\"\n",
    "    Reconstructs an audio waveform from a 64x64 image.\n",
    "    \n",
    "    This baseline method uses ONLY the V (Value) channel \n",
    "    as a Mel Spectrogram and estimates phase.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Load Image ---\n",
    "        # Load the 64x64 RGB image\n",
    "        img_rgb = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # --- 2. Convert RGB -> HSV ---\n",
    "        # Convert to HSV so we can access the Value channel\n",
    "        hsv_image = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # --- 3. Extract the V (Value) Channel ---\n",
    "        # hsv_image[:, :, 2] is the V channel\n",
    "        # Values are 0-255. We normalize to 0.0-1.0 float.\n",
    "        V_map_norm = hsv_image[:, :, 2] / 255.0\n",
    "        \n",
    "        # --- 4. Un-normalize the V Channel ---\n",
    "        # In our forward script, V was a log-Mel spectrogram from librosa.amplitude_to_db,\n",
    "        # which maps to a range of approx. -80dB (silence) to 0dB (max).\n",
    "        # We now reverse that mapping.\n",
    "        # 0.0 -> -80dB\n",
    "        # 1.0 -> 0dB\n",
    "        V_map_db = (V_map_norm * 80.0) - 80.0\n",
    "        \n",
    "        # --- 5. Convert dB -> Amplitude ---\n",
    "        # This is our approximate 64x64 Mel spectrogram in amplitude\n",
    "        S_mel_approx = librosa.db_to_amplitude(V_map_db)\n",
    "        \n",
    "        # --- 6. Invert Mel Spectrogram -> Linear STFT ---\n",
    "        # This is the \"magic\" step. We ask librosa to \"best-guess\"\n",
    "        # what a full STFT (1025 x 64) would result in this\n",
    "        # 64x64 Mel spectrogram.\n",
    "        S_stft_approx = librosa.feature.inverse.mel_to_stft(\n",
    "            S_mel_approx, \n",
    "            sr=sr, \n",
    "            n_fft=n_fft\n",
    "        )\n",
    "        \n",
    "        # --- 7. Reconstruct Audio with Griffin-Lim ---\n",
    "        # This algorithm iteratively \"guesses\" the phase\n",
    "        # that is most consistent with the magnitude spectrogram.\n",
    "        print(\"   Running Griffin-Lim... (this can take a moment)\")\n",
    "        y_reconstructed = librosa.griffinlim(S_stft_approx)\n",
    "        \n",
    "        return y_reconstructed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Could not process image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- ğŸš€ HOW TO RUN ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # 1. Choose an image you just created\n",
    "    # ****** PLEASE UPDATE THIS PATH ******\n",
    "    image_to_test = \"hsv_images_64/rock/rock.00000.png\"\n",
    "    \n",
    "    # 2. Define where to save the new audio file\n",
    "    # ****** PLEASE UPDATE THIS PATH ******\n",
    "    output_audio_path = \"reconstructed_audio_test.wav\"\n",
    "    \n",
    "    # 3. Define the sample rate (must match your original audio)\n",
    "    SR = 22050 \n",
    "    \n",
    "    print(f\"Attempting to reconstruct audio from: {image_to_test}\")\n",
    "    \n",
    "    # Generate the audio\n",
    "    reconstructed_waveform = generate_audio_from_image(image_to_test, sr=SR)\n",
    "    \n",
    "    if reconstructed_waveform is not None:\n",
    "        # Save the audio\n",
    "        sf.write(output_audio_path, reconstructed_waveform, SR)\n",
    "        print(f\"\\nâœ… Success! Reconstructed audio saved to:\")\n",
    "        print(f\"   {output_audio_path}\")\n",
    "        print(\"\\nListen to it! Compare it to the original 'rock.00000.wav'.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Failed to generate audio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a539938d",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Attempting to reconstruct audio from: hsv_images_64/rock/rock.00013.png\n",
      "   Stretching 64 frames back to 1292 frames...\n",
      "   Running Griffin-Lim... (this will take longer now)\n",
      "\n",
      "âœ… Success! Reconstructed 30s audio saved to:\n",
      "   reconstructed_audio_test_30sec_rock_13.wav\n",
      "\n",
      "Listen to it! It will sound 'phasey', but the tempo should be correct.\n"
     ]
    }
   ],
   "source": [
    "import librosa\n",
    "import numpy as np\n",
    "import cv2\n",
    "import soundfile as sf\n",
    "from PIL import Image\n",
    "\n",
    "def generate_audio_from_image(image_path,\n",
    "                              audio_duration_sec=30,\n",
    "                              sr=22050,\n",
    "                              n_fft=2048,\n",
    "                              hop_length=512, # We still need this for Griffin-Lim\n",
    "                              grid_size=64):\n",
    "    \"\"\"\n",
    "    Reconstructs a full-duration audio waveform from a 64x64 image.\n",
    "    Stretches the time axis from 64 pixels back to its original frame count.\n",
    "    \"\"\"\n",
    "    \n",
    "    try:\n",
    "        # --- 1. Load Image ---\n",
    "        img_rgb = cv2.imread(image_path, cv2.IMREAD_COLOR)\n",
    "        \n",
    "        # --- 2. Convert RGB -> HSV ---\n",
    "        hsv_image = cv2.cvtColor(img_rgb, cv2.COLOR_RGB2HSV)\n",
    "        \n",
    "        # --- 3. Extract the V (Value) Channel ---\n",
    "        V_map_norm = hsv_image[:, :, 2] / 255.0\n",
    "        \n",
    "        # --- 4. Un-normalize the V Channel ---\n",
    "        V_map_db = (V_map_norm * 80.0) - 80.0\n",
    "        \n",
    "        # --- 5. STRETCH THE TIME AXIS ---\n",
    "        n_frames_original = int(np.ceil(audio_duration_sec * sr / hop_length))\n",
    "        \n",
    "        print(f\"   Stretching {grid_size} frames back to {n_frames_original} frames...\")\n",
    "        \n",
    "        V_map_db_stretched = cv2.resize(\n",
    "            V_map_db,\n",
    "            (n_frames_original, grid_size), # (width, height)\n",
    "            interpolation=cv2.INTER_LINEAR\n",
    "        )\n",
    "        \n",
    "        # --- 6. Convert dB -> Amplitude ---\n",
    "        S_mel_approx = librosa.db_to_amplitude(V_map_db_stretched)\n",
    "        \n",
    "        # --- 7. Invert Mel Spectrogram -> Linear STFT ---\n",
    "        # The hop_length argument is removed from this call\n",
    "        S_stft_approx = librosa.feature.inverse.mel_to_stft(\n",
    "            S_mel_approx,\n",
    "            sr=sr,\n",
    "            n_fft=n_fft\n",
    "            # hop_length=hop_length <-- THIS WAS THE BUG\n",
    "        )\n",
    "        \n",
    "        # --- 8. Reconstruct Audio with Griffin-Lim ---\n",
    "        print(\"   Running Griffin-Lim... (this will take longer now)\")\n",
    "        # We *do* need hop_length here\n",
    "        y_reconstructed = librosa.griffinlim(\n",
    "            S_stft_approx,\n",
    "            hop_length=hop_length \n",
    "        )\n",
    "        \n",
    "        return y_reconstructed\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"   [Error] Could not process image {image_path}: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- ğŸš€ HOW TO RUN ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    image_to_test = \"hsv_images_64/rock/rock.00013.png\"\n",
    "    output_audio_path = \"reconstructed_audio_test_30sec_rock_13.wav\"\n",
    "    SR = 22050\n",
    "    HOP_LEN = 512 # Define hop_length explicitly\n",
    "    \n",
    "    print(f\"Attempting to reconstruct audio from: {image_to_test}\")\n",
    "    \n",
    "    # Generate the audio\n",
    "    reconstructed_waveform = generate_audio_from_image(\n",
    "        image_to_test, \n",
    "        audio_duration_sec=30,\n",
    "        sr=SR,\n",
    "        hop_length=HOP_LEN\n",
    "    )\n",
    "    \n",
    "    if reconstructed_waveform is not None:\n",
    "\n",
    "        print(\"   Normalizing audio amplitude...\")\n",
    "        max_val = np.max(np.abs(reconstructed_waveform))\n",
    "        if max_val > 0:\n",
    "             reconstructed_waveform = reconstructed_waveform / max_val\n",
    "        \n",
    "        # Save the audio\n",
    "        sf.write(output_audio_path, reconstructed_waveform, SR)\n",
    "        print(f\"\\nâœ… Success! Reconstructed 30s audio saved to:\")\n",
    "        print(f\"   {output_audio_path}\")\n",
    "        print(\"\\nListen to it! It will sound 'phasey', but the tempo should be correct.\")\n",
    "    else:\n",
    "        print(\"\\nâŒ Failed to generate audio.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "c9e73d88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing directory: Data/images_original/rock/\n",
      "\n",
      "--- Analysis Complete ---\n",
      "âœ… Total Number of Images Found: 100\n",
      "âœ… Pixel-wise Dimensions (H x W x C): 288 x 432 x 3\n",
      "âœ… Total Number of Pixels (H * W * C): 373248 (data points)\n",
      "Analyzing directory: hsv_images_64/rock/\n",
      "\n",
      "--- Analysis Complete ---\n",
      "âœ… Total Number of Images Found: 100\n",
      "âœ… Pixel-wise Dimensions (H x W x C): 64 x 64 x 3\n",
      "âœ… Total Number of Pixels (H * W * C): 12288 (data points)\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "import cv2\n",
    "import numpy as np\n",
    "\n",
    "def analyze_image_directory(directory_path):\n",
    "    \"\"\"\n",
    "    Scans a directory to count all images, get the dimensions\n",
    "    of the first one, and calculate its total pixel count.\n",
    "    \n",
    "    Args:\n",
    "        directory_path (str): The path to the main folder \n",
    "                              (e.g., \"hsv_images_64/\").\n",
    "    \"\"\"\n",
    "    \n",
    "    total_images = 0\n",
    "    image_dimensions = None\n",
    "    total_pixels = 0\n",
    "    \n",
    "    print(f\"Analyzing directory: {directory_path}\\n\")\n",
    "    \n",
    "    if not os.path.isdir(directory_path):\n",
    "        print(f\"Error: Directory not found at '{directory_path}'\")\n",
    "        return\n",
    "\n",
    "    # os.walk will go through the main folder and all its sub-folders\n",
    "    for root, dirs, files in os.walk(directory_path):\n",
    "        for file_name in files:\n",
    "            # Check for common image extensions\n",
    "            if file_name.lower().endswith(('.png', '.jpg', '.jpeg', '.bmp')):\n",
    "                \n",
    "                # --- Count the image ---\n",
    "                total_images += 1\n",
    "                \n",
    "                # --- Get dimensions and pixel count of the first image ---\n",
    "                if image_dimensions is None:\n",
    "                    try:\n",
    "                        full_path = os.path.join(root, file_name)\n",
    "                        image = cv2.imread(full_path)\n",
    "                        \n",
    "                        if image is not None:\n",
    "                            # .shape returns (height, width, channels)\n",
    "                            h, w, c = image.shape\n",
    "                            image_dimensions = (h, w, c)\n",
    "                            \n",
    "                            # --- Calculate total pixels ---\n",
    "                            total_pixels = h * w * c\n",
    "                        else:\n",
    "                            print(f\"Warning: Could not read first image: {full_path}\")\n",
    "                            \n",
    "                    except Exception as e:\n",
    "                        print(f\"Error reading dimensions from {full_path}: {e}\")\n",
    "\n",
    "    # --- Print the final report ---\n",
    "    if total_images == 0:\n",
    "        print(\"No images found in that directory.\")\n",
    "    else:\n",
    "        print(\"--- Analysis Complete ---\")\n",
    "        print(f\"âœ… Total Number of Images Found: {total_images}\")\n",
    "        \n",
    "        if image_dimensions:\n",
    "            height, width, channels = image_dimensions\n",
    "            print(f\"âœ… Pixel-wise Dimensions (H x W x C): {height} x {width} x {channels}\")\n",
    "            print(f\"âœ… Total Number of Pixels (H * W * C): {total_pixels} (data points)\")\n",
    "        else:\n",
    "            print(\"âŒ Could not determine image dimensions.\")\n",
    "\n",
    "# --- ğŸš€ HOW TO RUN ---\n",
    "if __name__ == \"__main__\":\n",
    "    \n",
    "    # *** UPDATE THIS PATH ***\n",
    "    # This should be the main folder that CONTAINS \n",
    "    # your 'blues', 'rock', etc. sub-folders.\n",
    "    IMAGE_DIR = \"hsv_images_64/\" \n",
    "    \n",
    "    analyze_image_directory(\"Data/images_original/rock/\")\n",
    "    analyze_image_directory(\"hsv_images_64/rock/\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
